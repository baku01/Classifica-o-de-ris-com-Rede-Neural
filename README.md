# Classifica√ß√£o de √çris com Rede Neural üå∫

Este notebook implementa uma rede neural para classificar flores de √≠ris usando o conjunto de dados Iris do sklearn. O objetivo √© demonstrar passo a passo o processo de carregamento dos dados, pr√©-processamento, defini√ß√£o e treinamento da rede neural, al√©m da visualiza√ß√£o dos resultados.

## Passo a Passo

### 1. Carregamento dos Dados
O conjunto de dados Iris √© carregado usando a fun√ß√£o `datasets.load_iris()` do sklearn. Em seguida, selecionamos as duas primeiras caracter√≠sticas (comprimento e largura da s√©pala) para visualiza√ß√£o.

```python
from sklearn import datasets

iris_dataset = datasets.load_iris()
iris_features = [0, 1]
iris_data = iris_dataset.data[:, iris_features]
```

### 2. Visualiza√ß√£o dos Dados
Plotamos um gr√°fico de dispers√£o das caracter√≠sticas selecionadas, colorindo as diferentes classes de flores.

```python
import matplotlib.pyplot as plt

plt.scatter(iris_dataset.data[:, 0], iris_dataset.data[:, 1], c=iris_dataset.target)
plt.xlabel(iris_dataset.feature_names[0])
plt.ylabel(iris_dataset.feature_names[1])
plt.show()
```

### 3. Pr√©-processamento dos Dados
Normalizamos os dados usando `MinMaxScaler` do sklearn para garantir que todas as caracter√≠sticas estejam na mesma escala.

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
data = scaler.fit_transform(iris_data)
```

### 4. Defini√ß√£o da Rede Neural
Definimos uma rede neural simples usando o PyTorch. A rede consiste em uma camada oculta com ativa√ß√£o de LeakyReLU e uma camada de sa√≠da com ativa√ß√£o Softmax.

```python
from torch import nn

input_size = data.shape[1]
hidden_size1 = 50
out_size = len(iris_dataset.target_names)

net = nn.Sequential(
    nn.Linear(input_size, hidden_size1),
    nn.LeakyReLU(),  
    nn.Linear(hidden_size1, out_size),
    nn.Softmax()
)
```

### 5. Treinamento do Modelo
Treinamos a rede neural usando a fun√ß√£o de perda de entropia cruzada e otimizador SGD (Gradiente Descendente Estoc√°stico). O treinamento √© feito em 1000 √©pocas.

```python
from torch import optim
import torch

# Definindo fun√ß√µes de otimiza√ß√£o
X = torch.FloatTensor(data)
Y = torch.LongTensor(iris_dataset.target)

# Fun√ß√£o de perda
criterion = nn.CrossEntropyLoss()

# Otimizador 
optimizer = optim.SGD(net.parameters(), lr=0.01, weight_decay=0.01)

# Treinamento
for i in range(1000):
    pred = net(X)
    loss = criterion(pred, Y)

    loss.backward()
    optimizer.step()
```

### 6. Visualiza√ß√£o da Classifica√ß√£o
A cada √©poca, plotamos a classifica√ß√£o das flores de √≠ris usando a fun√ß√£o `plot_sepal`, que mostra as fronteiras de decis√£o da rede neural.

```python
from torch import Tensor
import numpy

def plot_sepal(X, y, model):
    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1
    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1

    spacing = min(x_max - x_min, y_max - y_min) / 100

    XX, YY = numpy.meshgrid(numpy.arange(x_min, x_max, spacing),
                            numpy.arange(y_min, y_max, spacing))

    data = numpy.hstack((XX.ravel().reshape(-1, 1), YY.ravel().reshape(-1, 1)))

    db_prob = model(Tensor(data))
    clf = numpy.argmax(db_prob.cpu().detach().numpy(), axis=1)

    Z = clf.reshape(XX.shape)

    plt.contourf(XX, YY, Z, cmap='brg', alpha=0.5)
    plt.scatter(X[:, 0], X[:, 1], c=y, s=15, cmap='brg')
    plt.show()
```

### 7. Sum√°rio da Rede Neural
Exibimos um resumo da arquitetura da rede neural ap√≥s o treinamento.

```python
from torchsummary import summary

summary(net, input_size=(input_size,))
```

## Como Executar
- Execute o notebook linha por linha ou c√©lula por c√©lula para ver os resultados passo a passo.
- Certifique-se de ter uma GPU dispon√≠vel para treinamento mais r√°pido, caso contr√°rio, ajuste o c√≥digo para executar no CPU.
- Ajuste os par√¢metros do treinamento conforme necess√°rio, como taxa de aprendizado e n√∫mero de √©pocas.